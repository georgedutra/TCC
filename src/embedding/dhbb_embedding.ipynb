{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eee5ab86",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "\n",
    "Nesse notebook consta o processo de criar um ebedding do dhbb.\n",
    "\n",
    "Perceba que no diretório apontado para salvar o índice, serão criados dois arquivos: um índice .faiss (base vetorial) e um arquivo .pkl (metadados das chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bef99a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import yaml\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ee148d",
   "metadata": {},
   "source": [
    "### Configurações de embedding e diretórios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08ecee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"/disco/dhbb/text\" # Diretório com os verbetes\n",
    "INDEX_SAVE_PATH = \"/disco/indexes\" # Diretório para salvar a base vetorial\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\" # Modelo de embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5f5caf",
   "metadata": {},
   "source": [
    "## 1. Carregar arquivos .text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdef3ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_files = glob.glob(os.path.join(INPUT_DIR, \"**/*.text\"), recursive=True)\n",
    "\n",
    "if not text_files:\n",
    "    raise ValueError(f\"Nenhum arquivo .text encontrado em {INPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae3b3f9",
   "metadata": {},
   "source": [
    "Repare que o tamanho dos verbetes costuma ser bem inconsistente. É importante levar isso em consideração quando definimos os parâmetros para criação de Documents e chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce007487",
   "metadata": {},
   "source": [
    "## 2. Processar arquivos e criar objetos Document\n",
    "\n",
    "Utiliza-se os cabeçalhos YAML dos verbetes para gerar os metadados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06142d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for file_path in text_files:\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "\n",
    "            # Dividir o cabeçalho YAML do corpo do texto\n",
    "            if content.startswith(\"---\"):\n",
    "                parts = content.split(\"---\", 2)\n",
    "                if len(parts) >= 3:\n",
    "                    _, header, body = parts\n",
    "                    metadata = yaml.safe_load(header)\n",
    "                else:\n",
    "                    body = content\n",
    "                    metadata = {}\n",
    "            else:\n",
    "                body = content\n",
    "                metadata = {}\n",
    "            \n",
    "            metadata[\"source\"] = os.path.basename(file_path)\n",
    "            metadata[\"file_path\"] = file_path\n",
    "\n",
    "            \n",
    "            # Criar objeto Document\n",
    "            documents.append(Document(\n",
    "                page_content=content,\n",
    "                metadata=metadata\n",
    "            ))\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar {file_path}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1ab383",
   "metadata": {},
   "source": [
    "## 3. Dividir documentos em chunks\n",
    "É importante definir bem os parâmetros de chunk size e chunk overlap, pois estes tem forte impacto na criação do embedding:\n",
    "\n",
    "- chunk_size: Uma vez que cada verbete foi transformado num Document, cada Document é quebrado em chunks. Se $chunk\\_size = n$, então cada chunk terá no máximo $n$ caracteres.\n",
    "- chunk_overlap: Para evitar que a informação de uma chunk seja completamente descontextualizada, se $chunk\\_overlap=m$, os $m$ últimos caracteres da $i$-ésima chunk serão repetidos no início da $(i+1)$-ésima chunk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2314b567",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=7000,\n",
    "    chunk_overlap=500\n",
    ")\n",
    "\n",
    "split_docs = text_splitter.split_documents(documents)  # Método correto para documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc6ef25",
   "metadata": {},
   "source": [
    "## 4. Criar embeddings e índice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49d1c23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Índice criado com 12756 chunks em /disco/indexes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=split_docs,  # Já são objetos Document\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "os.makedirs(os.path.dirname(INDEX_SAVE_PATH), exist_ok=True)\n",
    "vectorstore.save_local(INDEX_SAVE_PATH)\n",
    "print(f\"Índice criado com {len(split_docs)} chunks em {INDEX_SAVE_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
